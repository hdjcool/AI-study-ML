{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---   \n",
    "\n",
    "# Part 1. Mini Project\n",
    "\n",
    "### 비즈니스 문제를 해결하기 위해 데이터로 어떻게 접근, 해결할지 생각해보는 문제\n",
    "    - Q. 1~3 문제 : 잔존율 증가를 위한 비즈니스 문제해결 \n",
    "    - Q. 4~6 문제 : 금융권 VIP 고객의 연간 소비금액을 예측하기 위한 모델 구축 작업\n",
    "    - Q. 7~8 문제 : 유저 세분화 및 그룹핑을 위한 데이터 분석 작업\n",
    "\n",
    "\n",
    "### 머신러닝의 학습과정 및 활용에 대한 이해를 확인하는 문제\n",
    "    - Q. 9 문제 : 손실함수의 필요성과 개념에 대한 이해\n",
    "    - Q. 10~11 문제 : Gradient Descent의 과정과 SGD, MGD 처리 방식의 이해\n",
    "    - Q. 12-13 문제 : 오버피팅 개념 및 해결방법에 대한 이해\n",
    "    - Q. 14-15 문제 : CNN 및 RNN 의 동작원리에 대한 이해와 활용\n",
    "\n",
    "\n",
    "- 작성자: 송훈화 감수자\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q. 1~3. 잔존율 증가를 위한 비즈니스 문제해결 방안 마련\n",
    "\n",
    "### 배경\n",
    "다음과 같은 비즈니스 문제가 있다고 하자. \n",
    "현재 앱서비스의 잔존율이 정체되어 있는 상황이며 재이용자의 증가가 필수적인 상황이다. 여기서 잔존율이란 서비스를 이용하던 기존 유저가 시간이 흘러도 지속적으로 이용하고 있는 정도를 의미한다. \n",
    "\n",
    "\n",
    "### 목표\n",
    "분석가에게 주어진 역할은 사내에 수집된 데이터를 추출해 잔존율을 높일 수 있는 방안을 유관팀에 공유하는 것이다. 유관팀은 개별 유저별로 잔존 여부를 예측할 수 있다면, 이를 근거로 개인화된 타깃팅을 진행할 수 있을 것이다. 분석가는 데이터를 근거로 유저들의 잔존 여부를 예측할 수 있는 모델을 구축하고자 한다. 즉 분석 목표는\n",
    "   - 잔존 vs 비잔존 그룹간의 유저 행동 패턴을 이해하고,\n",
    "   - 개별 유저의 잔존 여부(Y/N)를 예측할 수 있는 모델을 만드는 것이다.\n",
    "\n",
    "위에 주어진 목표를 달성하기 위해 어떤 접근방법을 활용해야할지 각 단계별로 기술해보자.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 수집: 어떤 데이터를 수집/추출할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 200자 이상 작성을 추천)\n",
    "\n",
    "사내 내부에서 수집된 잔존과 비잔존 그룹간의 일별, 월별, 연별 접속 Log 데이터를 수집하고\n",
    "이를 구분 할 수 있는 데이터를 분류(고객 성별, 고객 연령, 서비스에 머문 시간, 재방문 횟수 등)\n",
    "이를 가지고 입력 데이터와 타겟값을 추출함.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 탐색적 데이터 분석: 유저의 행동패턴 이해를 위해 데이터 탐색을 어떻게 할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 200자 이상 작성을 추천)\n",
    "\n",
    "탐색적 데이터 분석은 변수 하나 혹슨 변수와 변수와의 관계 등 데이터 자체의 특성을 확인하기 위한 분석으로\n",
    "분석목표가 확실하더라도 데이터를 더 확실히 이해하기 위해서 이 과정은 꼭 필요함.\n",
    "변수나 변수 관계에 대한 열린 분석 실행\n",
    "요약은 데이터를 압축하는 과정으로 데이터의 정보를 인식 가능한 수준으로 줄이는 과정\n",
    "탐색적 데이터 분석은 요약으로 대부분 끝나며\n",
    "그룹별로 평균을 내는 작업등을 말함.(평균, 최대값 계산 등 단순 숫자요약)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 모형 적합: 어떤 예측모형을 이용해 잔존 여부를 예측할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 200자 이상 작성을 추천)\n",
    "\n",
    "입력 데이터와 타겟값을 가지고 데이터를 학습하여 관계를 모델링 하는 지도 학습중\n",
    "타겟변수 Y 가 이산형 변수(잔존그룹/비잔존그룹) 인 경우 사용할 수 있는 분류(Classfification)를 사용함\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q. 4~6. 금융권 VIP 고객의 연간 소비금액을 예측하기 위한 모델 구축 작업\n",
    "\n",
    "### 배경 및 목표\n",
    "금융권의 한 기업에서 VIP 고객들의 연간 소비금액(단위: 원)을 예측하기 위한 모델을 만들고 있다. 영업팀은 이 예측모델을 새로운 고객관리시스템에 도입하고자 준비하고 있다. 분석가의 목표는 기존 VIP 고객들의 소비금액을 기반으로 새로운 VIP 고객의 연간 소비금액(단위: 원)을 예측하는 것이다.\n",
    "\n",
    "### 데이터셋\n",
    "분석가에게 주어진 데이터셋의 컬럼은 아래와 같다. \n",
    "\n",
    "- 고객아이디(숫자형)\n",
    "- 연봉(숫자형)\n",
    "- 주소(문자) \n",
    "- 연간 소비금액 (숫자형, 단위: 원)\n",
    "- 성별(문자)\n",
    "- 계좌 잔고금액(숫자형, 단위: 원)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 입력 데이터(X, features)로 적절한 변수와 타깃 데이터(y, target, label)로 적절한 변수는 각각 무엇일까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 100자 이상 작성을 추천)\n",
    "\n",
    "\n",
    "### 입력데이터\n",
    "1. 연봉\n",
    "2. 성별\n",
    "3. 계좌 잔고금액\n",
    "\n",
    "### 타깃데이터\n",
    "1. 연긴 소비금액\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 지도학습(회귀), 지도학습(분류), 비지도학습, 강화학습 중에 어떤 모델을 적용할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 100자 이상 작성을 추천)\n",
    "\n",
    "입력 데이터와 타겟값을 알고 있는 데이터를 학습하여 이들의 관계를 모델링하는 학습방법인 지도학습 중 \n",
    "타겟변수 Y 가 연속형 변수인 경우 활용 할 수 있는  회귀(Regression) 모델을 이용 함.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 생성된 모델이 학습 데이터에서는 성능이 높았으나 테스트 데이터에서 성능이 낮았다. 추정되는 이유는 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 200자 이상 작성을 추천)\n",
    "\n",
    "학습 과정에서 하이퍼파라미터를 튜닝하는데 사용 되는 검증셋 부분에서 데이터 포인트 수가 충분하지 않아\n",
    "최적의 하이퍼파라미터 값을 선정 하지 못하여 테스트 셋으로 모델을 평가하는 과정에서 성능이 나오지 않았을 것이라 예측 됨.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7~8.  유저 세분화 및 그룹핑을 위한 데이터 분석 작업\n",
    "\n",
    "### 배경 및 목표\n",
    "전체 소비자를 대상으로 한 마케팅 비용 및 리소스가 매우 큰 것으로 나타남에 따라, 전체 소비자를 세분화하여 그룹을 만든후 특정 그룹을 대상으로 마케팅을 진행하고자 한다. 분석가의 역할은 주어진 아래 데이터셋을 가지고 소비자를 세분화된 결과를 마케팅팀에 공유하는 것이다.\n",
    "\n",
    "### 데이터셋\n",
    "\n",
    "- 유저 아이디(숫자형)\n",
    "- 방문당 평균 결제횟수 (숫자형)\n",
    "- 방문당 공유 횟수 (숫자형)\n",
    "- 재방문율 (숫자형)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 지도학습(회귀), 지도학습(분류), 비지도학습, 강화학습 중에 어떤 모델을 적용할 것인가? 이유는 무엇인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 150자 이상 작성을 추천)\n",
    "\n",
    "데이터셋으로 주어진 방문당 평균 결제횟수, 방문당 공유 횟수, 재방문율 등은 모두 입력값이며 \n",
    "이러한 입력에 대한 타겟값이 없기 때문에 스팸 메일 분류나 개와 고양이 분류 처럼 입력 데이터와 타겟값을 알고 있는 데이터를 학습하여 이들의 관계를 모델링하는 학습방법인 지도학습은 배제해야 될 것으로 생각 됨.\n",
    "따라서 입력 데이터의 내재 되어 있는 특성을 찾아내기 위하여 타겟값(Y)이 없는 입력 데이터(X)만을 이용하여 학습하는 \n",
    "비지도 학습을 적용해야 하며 비지도 학습 종류 중 유사한 포인트들끼리 그룹을 만드는 군집화(Clustering)가 가장 적당할 것으로 생각 됨\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 만약 위 변수를 가지고 명확히 소비자가 세분화되지 않는다면 어떻게 해결하는 것이 좋을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 150자 이상 작성을 추천)\n",
    "\n",
    "기존에 가지고 있는 데이터셋으로 먼저 타겟값을(이때 타겟값은 특정한 값만 가질 수 있는 이산형 변수) 특정하고 \n",
    "이것을 새로운 데이터셋으로 활용하여 입력 데이터와 타겟값을 알고 있는 데이터를 학습하여 \n",
    "이들의 관계를 모델링하는 학습방법인 지도학습 중 타겟변수 Y 가 이산형 변수인 경우 활용 할 수 있는 \n",
    "분류(Classifictaion) 모델을 이용하여 전체 소비자를 세분화하여 그룹을 만든 후 특정 그룹 찾아 낸다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 9. 머신러닝에서 손실함수는 모델의 학습과정에서 매우 중요한 역할을 한다. 이 역할에 대해 상세히 기술해보자. 그리고 회귀, 분류 각 문제별로 대표적인 손실함수를 예로 들어보자.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 200자 이상 작성을 추천)\n",
    "\n",
    "- 손실 함수는 학습 알고리즘이 작동하게끔 만드는 원동력이며 이러한 손실함수의 값을 줄여나가는 과정이 곧 모델을 학습하는 과정이다.\n",
    "- 손실(Loss)은 실제 데이터에서 관측된 결과 vs 모델에 의해 생성된 결과 둘의 차이에 의해 손실이 발생\n",
    "- 손실이 작으면 모델 성능이 좋다라고 할 수 있다.\n",
    "- 이러한 손실은 수리적 계산을 위하여 함수로 만들어야 한다.\n",
    " - $L = f(y, yhat) = f(y,g(x,θ))$\n",
    " - f : loss function, y : training data, g : model, θ : model parameters\n",
    "\n",
    "- 손실함수의 종류\n",
    " - 교차 엔트로피\n",
    " - 평균 제곱 오차\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10~11. Gradient Descent의 과정과 SGD, MGD 처리 방식의 이해\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Gradient Descent 는 손실함수를 최소화하는 Weight을 찾아내기 위해 점진적으로 진행하는 최적화 방법중 하나이다. 경사하강법을 통해 손실함수 값을 최소화하는 과정을 간단히 기술해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 400자 이상 작성을 추천)\n",
    "\n",
    "- weight의 초기값은 일단 Random 한 값으로 시작\n",
    "- Neural Network 이 얼마나 못하는지에 대한 척도가 필요\n",
    "- 많이 쓰는 방법 중 한가지는 Neural Network 의 출력과 실제 정답의 차이 제곱\n",
    "- Loss Function 을 줄이들도록 weight 값들을 조금씩 바꾸는것을 미분을 이용하여 찾아 냄\n",
    "- Gradient Descent는 Loss Function 의 미분(Gradient)을 이용하여 weight 를 update 하는 방법\n",
    "- Loss 를 w로 미분하고 미분값이 가리키는 방향의 반대방향으로 아주 조금씩 w를 변경하면 Loss를 감소 시킬 수 있다.\n",
    "- 모든 training data 에 대하여 Neural Network 의 출력과 실제값을 비료하여 각각의 Loss 를 계산하고 모두 더해 전체 Loss 를 계산\n",
    "  이 Loss 를 weight로 미분한다음 그 미분값이 가리키는 반대방향으로 weight 값을 아주 조금씩 변경\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. 많은 데이터에 대해 한번에 Gradient Descent 를 적용했을 때(즉 Batch 처리) 학습에 시간이 오래 걸리는 문제가 발생한다. 따라서 Stochastic, Mini-batch Gradient Descent 과 같은 방법들을 사용하는데, 각 방법들에 대해 기술해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 200자 이상 작성을 추천)\n",
    "\n",
    "- 전체 data 중 일부 data 만 sampling\n",
    "- 이 data 들이 전체 training data 를 대표한다고 가정\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "- 전체 data 중 data 1개만 sampling 하여 그 data 의 Loss 를 이용하여 Gradient Descent\n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "- Batch 와 Stochastic 중간 형태로 data를 n개 sampling\n",
    "- 그 n 개의 data 에 대한 Loss 를 계산하여 다 더한 뒤 이를 이용하여 Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12~13. 오버피팅 개념 및 해결방법에 대한 이해\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. 머신러닝 모델을 구축하기 위해 전체 데이터를 학습셋, 검증셋, 테스트셋으로 분리시키는데 이렇게 분리시키는 목은 무엇이며, 각 데이터셋의 이용 목적을 기술해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 300자 이상 작성을 추천)\n",
    "\n",
    "### 학습 셋 \n",
    "- 모델 생성을 위해 학습 과정에 사용\n",
    "- 모델 파라미터 추정을 위해 소모됨\n",
    "\n",
    "### 검증/개발 셋\n",
    "- 학습 과정에서 하이퍼파라미터를 튜닝하는데 사용\n",
    "- 여러 하이퍼파라미터로 생성된 모델 중 어떤 것이 성능이 좋은지 평가하는데 소모됨\n",
    "\n",
    "### 테스트 셋\n",
    "- 생성된 모델의 예측 성능 평가\n",
    "- 미래에 타겟값이 관측되지 않은 데이터라고 가정하고, 예측이 잘 되는지 평가하는 데에 소모됨\n",
    "\n",
    "### 3-way holdout 방법\n",
    "1. 갖고 있는 데이터를 3개로 분할(학습, 개발, 테스트 셋)\n",
    "2. 여러 하이퍼파라미터 셋으로 후모 모델들을 학습\n",
    "3. 검증(validation) set 을 이용하여 모델들을 평가\n",
    "4. 학습셋과 검증셋을 합쳐서 앞서 도출된 best 하이퍼파라미터 셋으로 모델을 재학습\n",
    "5. Test set 으로 모델을 평가\n",
    "6. 모든 데이터로 최종 모델 학습 -> 이 모델을 미래 예측에 사용\n",
    "\n",
    "테스트 셋은 모델 학습과 튜닝에 전혀 이용되지 않으므로 향후 발생할 데이터처럼 타겟 관측이 되지 않았다고 가정하여 평가 가능\n",
    "따라서 타겟값이 이미 있는 데이터를 학습셋과 검증셋으로 분할하여 모델을 학습 및 튜닝해서 나온 결과를 테스트 셋으로 평가하면 \n",
    "결국 Y_hat 값을 가질 수 있고 이를 Y 와 비교해 모델의 성능을 추정하고 이 성능이 좋다면 새로운 데이터에 대해서도 성능이 잘 나올것으로 기대할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.  과적합(Overftting)이 발생했을 때 이를 해결할 수 있는 방안은 무엇이 있을지 작성해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 100자 이상 작성을 추천)\n",
    "\n",
    "- Overfitting : Training data 에 너무 최적화(fitting)을 하여 일반화(generalization) 성능이 떨어지는 현상\n",
    "- test error = training error + generalization gap\n",
    "- model capacity(뉴럴네트워크의 레이어의 갯수라고 봐도 됨)가 너무 작으면 아무리 학습을 해도 성능이 안나옴 : Underfitting\n",
    "- model capacity 가 너무 크면 generalization gap 이 커짐 : Overfitting\n",
    "- 딥러닝에 사용하는 model 은 capacity 가 대부분 커서 Overfitting 에 취약\n",
    "- Overfitting 이 되면 융통성이 없어짐\n",
    "\n",
    "### Overfitting 을 막는 방법\n",
    "1. data 양을 늘림\n",
    "2. 정규화(regularization)를 통한 과적합(Overfitting) 방지\n",
    " - L1/L2 Regularization(Weight Decay)\n",
    " - Dropout\n",
    " - Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14~15. CNN 및  RNN 의 동작원리에 대한 이해와 활용\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 14. 주로 이미지 인식을 위해 가장 널리 알려진 CNN(Convolutional Neural Network)의 학습과정에 대해 간단히 기술해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 200자 이상 작성을 추천)\n",
    "\n",
    "### convolution layer, pooling layer, fully-connected layer로 구성\n",
    "- convolution layer - Feature extraction(추출)\n",
    "- pooling layer - 추출된 Feature 들을 모음(중요한 것들만 뽑음)\n",
    "- fully-connected layer - 최종적으로 모임 feature 들을 가지고 class 를 판단\n",
    "\n",
    "### CNN 의 동작 원리\n",
    "- 이미지를 작은 tile 로 나누고, 첫번째 tile 에서 특정 feature 추출(예: 귀)\n",
    "- 다음 tile 로 이동하면서 같은 방법으로 feature 를 추출(동일한 weight 사용)\n",
    "- 다른 feature(예: 눈)를 추출하는 filter 가 위와 같은 방법으로 tile 을 하나씩 이동하며 feature 추출\n",
    "- 결과를 다음 layer 로 보내서 계속 반복\n",
    "- 최종적으로 추출된 모든 feature 들을 잘 조합하여 최종적으로 이미지를 판단\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. RNN(Recurrent Neural Network)의 개념과 대표적인 문제점, 해결방법에 대해 기술해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# markdown 으로 작성 (공백포함 / 200자 이상 작성을 추천)\n",
    "\n",
    "### RNN\n",
    "- 내부적으로 순환이 되는 구조를 이용 순서가 있는 데이처를 처리하는데 적합한 알고리즘\n",
    "- 앞뒤 문맥을 가지고 있는 데이처 처리에 사용\n",
    "- 현재 시간의 데이터가 앞뒤 데이터와 연관관계를 가지고 있는 시계열 데이처를 다룰 때 유용\n",
    "\n",
    "### 문제점\n",
    "1. Sequence 가 길어지면 성능이 떨어짐\n",
    "2. Sequence 가 길어지면 학습이 잘 단됨\n",
    "3. 오래전 입력에 대해서 기억을 잘 못함\n",
    "즉 순서상의 갭이 커질수록 정보를 연결하기 힘든 단점이 있음\n",
    "\n",
    "### 해결방법\n",
    "- LSTM(Long Short Term Memory) 은 장기의존성 문제를 해결하기 위해 새롭게 만들어짐.\n",
    "- 일련의 네트워크를 만들어서 이전의 데이터를 넘겨줄지 삭제할지 컨트롤 할 수 있게 함.\n",
    "- GRU(Gated Recurrent Unit) 은 LSTM 의 일부 상태와 게이트를 결합하여 보다 단순한 구조로 변형 한 것.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
